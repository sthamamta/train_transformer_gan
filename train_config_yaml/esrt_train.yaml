#configuration file


model_name: 'esrt'
exp_name: 'esrt_factor_1_l1_ph1_raw_array_train'
project_name: 'transformer_gan'
wandb: True 


#dataset class setting
# train_label_dir: '../model_bias_experiment/mri_dataset_25/train'
train_label_dir: '../array_data/25'
eval_label_dir: '../model_bias_experiment/mri_dataset_25/val'
patch_size: 120  # this is the patch size of hr therefore the lr patch size will be hr_patch_size/factor
lr_patch_size: 120
factor: 1

#bicubic, nearest, bilinear, lanczos,kspace,kspace_gaussian_100,mean_blur,median_blur,hanning,hamming
# downsample_method: ['bicubic','nearest','bilinear','lanczos','kspace','kspace_gaussian_100','hanning', 'hamming', 'median_blur', 'mean_blur']  
downsample_method: ['bicubic','kspace_gaussian_100','hanning', 'hamming', 'median_blur', 'mean_blur'] 
# downsample_method: ['bicubic'] 
augment: True
normalize: True

#training settings
num_epochs: 3505
n_freq: 100 # model checkpoint saving frequency
plot_train_example: True
train_batch_size: 8
num_workers: 8
seed: 547

# eval setting
eval: False
eval_freq: 100
eval_batch_size: 1   #set the val batch size to 1 always as we need to claculate the psnr and ssim


resume: False
# if resume True then
# checkpoint_path: 
# start_epoch: 

# gen_loss_type: ['l1', 'ssim', 'tv_regularizer', 'laplacian_pyramid']
gen_loss_type: ['l1']
gen_loss_wt: [1.0]


#to set the save directory
patch: True

# esrt generator setting
n_feats: 16
n_blocks: 1
kernel_size: 3


#optimizer for generator 
g_lr: 0.00002     # 0.001
g_optimizer_type: 'adam'
g_lr_decay_factor: 0.95
g_lr_schedular_step: 200
g_optimizer_betas: (0.99, 0.999)
