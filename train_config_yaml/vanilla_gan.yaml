#configuration file


model_name: 'esrt_gan'
exp_name: 'esrt_gan_factor_2_ssim_loss_bicubic_test'
project_name: 'transformer_gan'
wandb: True 


#dataset class setting
train_label_dir: '../model_bias_experiment/mri_dataset/train'
eval_label_dir: '../model_bias_experiment/mri_dataset/val'
patch_size: 200  # this is the patch size of hr therefore the lr patch size will be hr_patch_size/factor
lr_patch_size: 100
factor: 2

#bicubic, nearest, bilinear, lanczos,kspace,kspace_gaussian_100,mean_blur,median_blur,hanning,hamming
downsample_method: ['bicubic','nearest','bilinear','lanczos','kspace','kspace_gaussian_100','hanning', 'hamming']  
augment: True
normalize: True

#training settings
num_epochs: 15
n_freq: 1 # model checkpoint saving frequency
plot_train_example: True
train_batch_size: 8
num_workers: 8
seed: 547


g_lr: 0.0001
d_lr: 0.0001
original_lr: 0.0001
g_optimizer_type: 'adam'
d_optimizer_type: 'rmsprop'

g_lr_decay_factor: 0.95
g_lr_schedular_step: 50

d_lr_decay_factor: 0.95
d_lr_schedular_step: 50

# eval setting
eval: False
eval_freq: 100
eval_batch_size: 1   #set the val batch size to 1 always as we need to claculate the psnr and ssim


resume: False
# if resume True then
# checkpoint_path: 
# start_epoch: 

gen_loss_type: ['l1']
gen_loss_wt: [1.0]
adversarial_weight: 1.0


#to set the save directory
patch: True

# esrt generator setting
n_feats: 16
n_blocks: 1
kernel_size: 3

# transformer discriminator setting
dist_patch_size: 10
in_chans: 1
num_classes: 1
embed_dim: 500
num_heads: 4
mlp_ratio: 4.
qkv_bias: False
qk_scale: None
drop_rate: 0.
attn_drop_rate: 0.
drop_path_rate: 0.
norm_layer: 'ln'
depth: 4
act_layer : 'gelu'
diff_aug: 'None'


#optimizer for generator 
g_lr: 0.001
g_optimizer_type: 'adam'
g_lr_decay_factor: 0.95
g_optimizer_betas: (0.99, 0.999)


# optimizer for discriminator
d_lr: 0.0001
d_optimizer_type: 'adam'
d_lr_decay_factor: 0.95
d_optimizer_betas: (0.99, 0.999)



