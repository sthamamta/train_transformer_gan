#configuration file


model_name: 'esrt_gan'
exp_name: 'esrt_factor_2_l1_ssim_tv_regularizer_all_degradation'
project_name: 'transformer_gan'
wandb: True 


#dataset class setting
train_label_dir: '../model_bias_experiment/mri_dataset_25/train'
eval_label_dir: '../model_bias_experiment/mri_dataset_25/val'
patch_size: 200  # this is the patch size of hr therefore the lr patch size will be hr_patch_size/factor
lr_patch_size: 100
factor: 2

#bicubic, nearest, bilinear, lanczos,kspace,kspace_gaussian_100,mean_blur,median_blur,hanning,hamming
downsample_method: ['bicubic','nearest','bilinear','lanczos','kspace','kspace_gaussian_100','hanning', 'hamming']  
augment: True
normalize: True

#training settings
num_epochs: 2505
n_freq: 100 # model checkpoint saving frequency
plot_train_example: True
train_batch_size: 8
num_workers: 8
seed: 547

# eval setting
eval: False
eval_freq: 100
eval_batch_size: 1   #set the val batch size to 1 always as we need to claculate the psnr and ssim


resume: False
# if resume True then
# checkpoint_path: 
# start_epoch: 

gen_loss_type: ['l1', 'ssim', 'tv_regularizer']
gen_loss_wt: [1.0, 1.0, 1.0]


#to set the save directory
patch: True

# esrt generator setting
n_feats: 16
n_blocks: 1
kernel_size: 3


#optimizer for generator 
g_lr: 0.0002     # 0.001
g_optimizer_type: 'adam'
g_lr_decay_factor: 0.90
g_lr_schedular_step: 200
g_optimizer_betas: (0.99, 0.999)
