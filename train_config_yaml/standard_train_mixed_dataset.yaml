#configuration file, with this file onlt adversarial training is possible

info: 'Training standard gan phase (2,3) by loading the pretrained model from phase 1 and mixed dataset'
model_name: 'esrt_gan'
exp_name: 'esrt_factor_1_standard_gan(adv)_chckpoint(ph1,l1)_mixed_dataset'
project_name: 'transformer_gan'
wandb: True 


#dataset class setting
real_input_dir: '../model_bias_experiment/mri_dataset_50/train'
real_label_dir: '../model_bias_experiment/mri_dataset_25/train'
train_dictionary_path: '../model_bias_experiment/mri_dataset_50/lr_hr_dictionary.pkl'
simulated_label_dir: '../model_bias_experiment/mri_dataset_25/train'
patch_size: 120  # this is the patch size of hr therefore the lr patch size will be hr_patch_size/factor
lr_patch_size: 120
factor: 1

downsample_method: ['bicubic','kspace_gaussian_100','hanning', 'hamming', 'mean_blur', 'median_blur']  


gan_loss_type: 'standard'

augment: False
normalize: True

#training settings
num_epochs: 2505
n_freq: 50 # model checkpoint saving frequency
plot_train_example: True
train_batch_size: 8
num_workers: 8
seed: 547
d_iter: 2  #changed this from 1 to 5


# eval setting
eval: False
eval_freq: 100
eval_batch_size: 1   #set the val batch size to 1 always as we need to claculate the psnr and ssim


resume: True
# if resume True then
checkpoint_path: 'outputs/esrt/esrt_factor_1_l1_ph1/checkpoints/patch/patch-120/factor_1/epoch_1200_f_1.pth'
start_epoch: 0
use_pixel_loss: False

# gen_loss_type: ['ranker_loss']
# gen_loss_wt: [1.0]
# ranker_checkpoint_path: 'outputs/transformer_classifier/transformer_classifier_ranking_ph123hr/checkpoints/patch/patch-200/epoch_3500_f_2.pth'
adversarial_weight: 1.0

#to set the save directory
patch: True

# esrt generator setting
n_feats: 16
n_blocks: 1
kernel_size: 3

# transformer discriminator setting
dist_patch_size: 10
in_chans: 1
num_classes: 1
embed_dim: 500
num_heads: 4
mlp_ratio: 4.
qkv_bias: False
qk_scale: None
drop_rate: 0.
attn_drop_rate: 0.
drop_path_rate: 0.
norm_layer: 'ln'
depth: 4
act_layer : 'gelu'
diff_aug: 'None'
apply_sigmoid: True


#optimizer for generator 
g_lr: 0.00002
g_optimizer_type: 'adam'
g_lr_decay_factor: 0.98
g_optimizer_betas: (0.99, 0.999)
g_lr_schedular_step: 200

# optimizer for discriminator
d_lr: 0.00001
d_optimizer_type: 'adam'
d_lr_decay_factor: 0.98
d_optimizer_betas: (0.99, 0.999)
d_lr_schedular_step: 200