#configuration file

info: 'Training standard gan phase 3 by loading the pretrained model from phase 2'
model_name: 'esrt_gan'
exp_name: 'esrt_factor_1_standard_gan(adv)_chckpoint(ph1,l1)_25_50_micron_array_data'
project_name: 'transformer_gan'
wandb: False 


#dataset class setting
train_input_path: '../array_data/50/'
train_label_path: '../array_data/25/'
train_dictionary_path: '../model_bias_experiment/mri_dataset_50/lr_hr_dictionary.pkl'
patch_size: 120  # this is the patch size of hr therefore the lr patch size will be hr_patch_size/factor
lr_patch_size: 120
factor: 1

gan_loss_type: 'standard'

augment: False
normalize: True
upsample_image: True

#training settings
num_epochs: 3505
n_freq: 50 # model checkpoint saving frequency
plot_train_example: True
train_batch_size: 8
num_workers: 8
seed: 547
d_iter: 2  #changed this from 1 to 5


# eval setting
eval: False
eval_freq: 100
eval_batch_size: 1   #set the val batch size to 1 always as we need to claculate the psnr and ssim


resume: True
# if resume True then
checkpoint_path: 'outputs/esrt/esrt_factor_1_l1_ph1_raw_array_train/checkpoints/patch/patch-120/factor_1/epoch_3500_f_1.pth'
start_epoch: 0
use_pixel_loss: True

gen_loss_type: ['content_loss', 'tv_regularizer']
gen_loss_wt: [1.0, 0.001]
# style_loss_weight: 1.0
# content_loss_weight: 1.0
# ranker_checkpoint_path: 'outputs/transformer_classifier/transformer_classifier_ranking_ph123hr/checkpoints/patch/patch-200/epoch_3500_f_2.pth'
adversarial_weight: 1.0

#to set the save directory
patch: True

# esrt generator setting
n_feats: 16
n_blocks: 1
kernel_size: 3

# transformer discriminator setting
dist_patch_size: 10
in_chans: 1
num_classes: 1
embed_dim: 500
num_heads: 4
mlp_ratio: 4.
qkv_bias: False
qk_scale: None
drop_rate: 0.
attn_drop_rate: 0.
drop_path_rate: 0.
norm_layer: 'ln'
depth: 4
act_layer : 'gelu'
diff_aug: 'None'
apply_sigmoid: True


#optimizer for generator 
g_lr: 0.00002
g_optimizer_type: 'adam'
g_lr_decay_factor: 0.9
g_optimizer_betas: (0.99, 0.999)
g_lr_schedular_step: 200

# optimizer for discriminator
d_lr: 0.00001
d_optimizer_type: 'adam'
d_lr_decay_factor: 0.9
d_optimizer_betas: (0.99, 0.999)
d_lr_schedular_step: 200